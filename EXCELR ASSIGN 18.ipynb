{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d712a291",
   "metadata": {},
   "source": [
    "## Name : Snehal  shyam Jagtap\n",
    "\n",
    "## Assignment No 18\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3b4d7",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72415e2a",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da33b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c9b71",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f480764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Alphabets_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01ab8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  letter  xbox  ybox  width  height  onpix  xbar  ybar  x2bar  y2bar  xybar  \\\n",
      "0      T     2     8      3       5      1     8    13      0      6      6   \n",
      "1      I     5    12      3       7      2    10     5      5      4     13   \n",
      "2      D     4    11      6       8      6    10     6      2      6     10   \n",
      "3      N     7    11      6       6      3     5     9      4      6      4   \n",
      "4      G     2     1      3       1      1     8     6      6      6      6   \n",
      "\n",
      "   x2ybar  xy2bar  xedge  xedgey  yedge  yedgex  \n",
      "0      10       8      0       8      0       8  \n",
      "1       3       9      2       8      4      10  \n",
      "2       3       7      3       7      3       9  \n",
      "3       4      10      6      10      2       8  \n",
      "4       5       9      1       7      5      10  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  20000 non-null  object\n",
      " 1   xbox    20000 non-null  int64 \n",
      " 2   ybox    20000 non-null  int64 \n",
      " 3   width   20000 non-null  int64 \n",
      " 4   height  20000 non-null  int64 \n",
      " 5   onpix   20000 non-null  int64 \n",
      " 6   xbar    20000 non-null  int64 \n",
      " 7   ybar    20000 non-null  int64 \n",
      " 8   x2bar   20000 non-null  int64 \n",
      " 9   y2bar   20000 non-null  int64 \n",
      " 10  xybar   20000 non-null  int64 \n",
      " 11  x2ybar  20000 non-null  int64 \n",
      " 12  xy2bar  20000 non-null  int64 \n",
      " 13  xedge   20000 non-null  int64 \n",
      " 14  xedgey  20000 non-null  int64 \n",
      " 15  yedge   20000 non-null  int64 \n",
      " 16  yedgex  20000 non-null  int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 2.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "print(data.head())        # View first few rows\n",
    "print(data.info())        # Check for missing values and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761baae",
   "metadata": {},
   "source": [
    "## 3. Split Features (X) and Labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]  # Features (could contain categorical data)\n",
    "y = data.iloc[:, -1]   # Target labels (alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fa7fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns in X: Index(['letter'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any categorical columns in X\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical Columns in X: {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f5ef3",
   "metadata": {},
   "source": [
    "## 4. One-Hot Encoding of Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c997297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After One-Hot Encoding, shape of X: (20000, 41)\n"
     ]
    }
   ],
   "source": [
    "# Apply One-Hot Encoding to categorical features, if any\n",
    "if len(categorical_columns) > 0:\n",
    "    column_transformer = ColumnTransformer(transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ], remainder='passthrough')  # Keep other numerical columns as they are\n",
    "    X = column_transformer.fit_transform(X)\n",
    "    print(\"After One-Hot Encoding, shape of X:\", X.shape)\n",
    "else:\n",
    "    print(\"No categorical columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a781f",
   "metadata": {},
   "source": [
    "## 5. Label Encoding for Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83f84e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
     ]
    }
   ],
   "source": [
    "# Encode the target labels (if they are categorical, like letters)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(\"Unique classes in y:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30477f",
   "metadata": {},
   "source": [
    "## 6. Splitting Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb495ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75f18bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: X_train: (16000, 41), y_train: (16000,)\n",
      "Test Set: X_test: (4000, 41), y_test: (4000,)\n"
     ]
    }
   ],
   "source": [
    "#Check the shape of the splits\n",
    "print(f\"Training Set: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Test Set: X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abefe57",
   "metadata": {},
   "source": [
    "## 7. Normalizing Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c81d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the feature data (only for numerical columns)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7b41664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature data normalized.\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature data normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790fa55",
   "metadata": {},
   "source": [
    "## 8. Build the ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3043026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73a7651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer and first hidden layer\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d967ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56fcf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer (using softmax for multiclass classification)\n",
    "model.add(Dense(units=len(np.unique(y)), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd262606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded1ffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "print(\"Model compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7bc4a5",
   "metadata": {},
   "source": [
    "## 9. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4efc888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "500/500 [==============================] - 4s 5ms/step - loss: 1.5261 - accuracy: 0.4572 - val_loss: 1.2507 - val_accuracy: 0.5242\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 1.1637 - accuracy: 0.5567 - val_loss: 1.1102 - val_accuracy: 0.5798\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 1.0584 - accuracy: 0.5915 - val_loss: 1.0286 - val_accuracy: 0.6168\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.9962 - accuracy: 0.6147 - val_loss: 0.9956 - val_accuracy: 0.6265\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.9515 - accuracy: 0.6302 - val_loss: 0.9642 - val_accuracy: 0.6255\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.9178 - accuracy: 0.6409 - val_loss: 0.9349 - val_accuracy: 0.6373\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.8909 - accuracy: 0.6511 - val_loss: 0.9173 - val_accuracy: 0.6432\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.8660 - accuracy: 0.6596 - val_loss: 0.9044 - val_accuracy: 0.6572\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.8488 - accuracy: 0.6665 - val_loss: 0.8891 - val_accuracy: 0.6557\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.8294 - accuracy: 0.6732 - val_loss: 0.8827 - val_accuracy: 0.6510\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.8148 - accuracy: 0.6801 - val_loss: 0.8645 - val_accuracy: 0.6603\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.8022 - accuracy: 0.6798 - val_loss: 0.8575 - val_accuracy: 0.6578\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7882 - accuracy: 0.6895 - val_loss: 0.8430 - val_accuracy: 0.6745\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7776 - accuracy: 0.6923 - val_loss: 0.8416 - val_accuracy: 0.6722\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7678 - accuracy: 0.6983 - val_loss: 0.8283 - val_accuracy: 0.6735\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7578 - accuracy: 0.6994 - val_loss: 0.8316 - val_accuracy: 0.6730\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7487 - accuracy: 0.7029 - val_loss: 0.8249 - val_accuracy: 0.6768\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7406 - accuracy: 0.7021 - val_loss: 0.8314 - val_accuracy: 0.6752\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7345 - accuracy: 0.7077 - val_loss: 0.8220 - val_accuracy: 0.6770\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7256 - accuracy: 0.7116 - val_loss: 0.8224 - val_accuracy: 0.6770\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7204 - accuracy: 0.7148 - val_loss: 0.8134 - val_accuracy: 0.6777\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7150 - accuracy: 0.7144 - val_loss: 0.8117 - val_accuracy: 0.6787\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7059 - accuracy: 0.7157 - val_loss: 0.8059 - val_accuracy: 0.6810\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7029 - accuracy: 0.7191 - val_loss: 0.8080 - val_accuracy: 0.6750\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.7204 - val_loss: 0.8039 - val_accuracy: 0.6810\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.7234 - val_loss: 0.8061 - val_accuracy: 0.6780\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.7254 - val_loss: 0.8058 - val_accuracy: 0.6823\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.7282 - val_loss: 0.8120 - val_accuracy: 0.6775\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.7280 - val_loss: 0.7999 - val_accuracy: 0.6892\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.7276 - val_loss: 0.8000 - val_accuracy: 0.6845\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b0868",
   "metadata": {},
   "source": [
    "## 10. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48af6392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 3ms/step - loss: 0.8000 - accuracy: 0.6845\n",
      "Test Accuracy: 68.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad3ffe",
   "metadata": {},
   "source": [
    "## 11. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64bc8f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 3ms/step\n",
      "Confusion Matrix:\n",
      " [[   0    0    0    0    1    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   0    4    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   0    3    0    1    2    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   0    1    1    4   22    1    1    0    1    0    0    0    0    0\n",
      "     0]\n",
      " [   0    0    0    5   52   28    5    2    1    1    0    0    0    0\n",
      "     0]\n",
      " [   0    0    0    1   18  115   42   14    4    1    1    0    0    0\n",
      "     0]\n",
      " [   0    0    2    0    2   60  191   81   12    3    2    0    0    0\n",
      "     0]\n",
      " [   0    0    0    0    3   10   55  432  204    4    3    1    0    0\n",
      "     0]\n",
      " [   0    0    0    0    2    4    9  103 1394   63   17    3    1    0\n",
      "     0]\n",
      " [   0    0    0    1    0    1    2   15  144  235   81    4    1    1\n",
      "     0]\n",
      " [   0    0    0    0    0    0    1    4   18   63  188   33    1    0\n",
      "     0]\n",
      " [   0    0    0    0    1    1    0    1    3    5   47  108    7    2\n",
      "     0]\n",
      " [   0    0    0    0    0    0    0    1    3    1    6    8   10    0\n",
      "     0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    2    2    5\n",
      "     0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
      "     0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         4\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.33      0.13      0.19        31\n",
      "           4       0.50      0.55      0.53        94\n",
      "           5       0.52      0.59      0.55       196\n",
      "           6       0.62      0.54      0.58       353\n",
      "           7       0.66      0.61      0.63       712\n",
      "           8       0.78      0.87      0.82      1596\n",
      "           9       0.62      0.48      0.55       485\n",
      "          10       0.54      0.61      0.58       308\n",
      "          11       0.68      0.62      0.65       175\n",
      "          12       0.45      0.34      0.39        29\n",
      "          13       0.56      0.56      0.56         9\n",
      "          14       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.68      4000\n",
      "   macro avg       0.45      0.46      0.45      4000\n",
      "weighted avg       0.68      0.68      0.68      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Display results\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9780c4",
   "metadata": {},
   "source": [
    "## 12. Plotting Training and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad37c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
